{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newstock.news import Newstock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_x, samsung_y = Newstock(name='삼성전자').get_data_by_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "class TimeBasedCV(object):\n",
    "    '''\n",
    "    Parameters \n",
    "    ----------\n",
    "    train_period: int\n",
    "        number of time units to include in each train set\n",
    "        default is 30\n",
    "    test_period: int\n",
    "        number of time units to include in each test set\n",
    "        default is 7\n",
    "    freq: string\n",
    "        frequency of input parameters. possible values are: days, months, years, weeks, hours, minutes, seconds\n",
    "        possible values designed to be used by dateutil.relativedelta class\n",
    "        deafault is days\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_period=30, test_period=7, freq='days'):\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.freq = freq\n",
    "\n",
    "        \n",
    "        \n",
    "    def split(self, data, validation_split_date=None, date_column='record_date', gap=0):\n",
    "        '''\n",
    "        Generate indices to split data into training and test set\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        data: pandas DataFrame\n",
    "            your data, contain one column for the record date \n",
    "        validation_split_date: datetime.date()\n",
    "            first date to perform the splitting on.\n",
    "            if not provided will set to be the minimum date in the data after the first training set\n",
    "        date_column: string, deafult='record_date'\n",
    "            date of each record\n",
    "        gap: int, default=0\n",
    "            for cases the test set does not come right after the train set,\n",
    "            *gap* days are left between train and test sets\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        train_index ,test_index: \n",
    "            list of tuples (train index, test index) similar to sklearn model selection\n",
    "        '''\n",
    "        \n",
    "        # check that date_column exist in the data:\n",
    "        try:\n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "                    \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "\n",
    "        if validation_split_date==None:\n",
    "            validation_split_date = data[date_column].min().date() + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        \n",
    "        start_train = validation_split_date - eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "        end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        while end_test <= data[date_column].max().date() + eval('relativedelta('+self.freq+'=1)'):\n",
    "            # train indices:\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "\n",
    "            # test indices:\n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            print(\"Train period:\",start_train,\"<= day <\" , end_train, \", Test period\", start_test, \"<= day <\", end_test,\n",
    "                  \"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "\n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "\n",
    "            # update dates:\n",
    "            start_train = start_train + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "            end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "            start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "            end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        # mimic sklearn output  \n",
    "        index_output = [(train,test) for train,test in zip(train_indices_list,test_indices_list)]\n",
    "\n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "    \n",
    "    \n",
    "    def get_n_splits(self):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, text='text'):\n",
    "        self.text = text\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Buy_Sell_transformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, cut_percent=0.01):\n",
    "        self.cut_percent = cut_percent\n",
    "\n",
    "    def fit(self, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = pd.cut(y, bins=[-np.inf, self.cut_percent, np.inf], labels=['nothing','buy'])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "y_process = Pipeline([\n",
    "    ('Buy_Sell_transformer', Buy_Sell_transformer()),\n",
    "])\n",
    "\n",
    "x_process = Pipeline([\n",
    "    ('selector', TextSelector()),\n",
    "    ('td_idf', TfidfVectorizer()),\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('process', x_process),\n",
    "                ('clf', MultinomialNB())\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input=141      [머니투데이 반준환 기자] [[특징주]]삼성전자가 1% 안팎의 반등세를 보이고 있다...\n",
       "142      81년을 이어온 삼성의 무(無)노조 경영이 깨지고 있다. 삼성전자(지난해 11월),...\n",
       "143      [서울경제] 코스피가 2일 나흘 만에 상승으로 출발했다. 이날 오전 9시 3분 코스...\n",
       "144      코스피 지수가 개인과 기관 매수세에 1990대 강세로 출발했다.2일 코스피 지수는 ...\n",
       "145      신종 코로나바이러스 감염증(코로나19) 공포에 지난 주 8.1% 폭락한 코스피지수가...\n",
       "                               ...                        \n",
       "17437    [이데일리 김윤지...\n",
       "17440    [서울경제] PC용 D램 현물가격이 최근 두달간 가파르게 하락하면서 ‘반도체 다운사...\n",
       "17441    (서울=뉴스1) 주성호 기자 = 서울 강남역에서 300일 넘게 고공농성을 벌였던 삼...\n",
       "Name: text, Length: 7793, dtype: object,\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer(samsung_x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2020-03-02 <= day < 2020-04-11 , Test period 2020-04-11 <= day < 2020-05-30 # train records 4072 , # test records 3721\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=40, test_period=49, freq='days')\n",
    "cv_sets = tscv.split(samsung_x.reset_index(drop=True), validation_split_date=None, date_column='timestamps', gap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('process', Pipeline(memory=None,\n",
       "            steps=[('selector', TextSelector(text='text')),\n",
       "                   ('td_idf',\n",
       "                    TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.float64'>,\n",
       "                                    encoding='utf-8', input='content',\n",
       "                                    lowercase=True, max_df=1.0, max_features=None,\n",
       "                                    min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                    preprocessor=None, smooth_idf=True,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    sublinear_tf=False,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=None, use_idf=True,\n",
       "                                    vocabulary=None))],\n",
       "            verbose=False)),\n",
       "  ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'verbose': False,\n",
       " 'process': Pipeline(memory=None,\n",
       "          steps=[('selector', TextSelector(text='text')),\n",
       "                 ('td_idf',\n",
       "                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.float64'>,\n",
       "                                  encoding='utf-8', input='content',\n",
       "                                  lowercase=True, max_df=1.0, max_features=None,\n",
       "                                  min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                  preprocessor=None, smooth_idf=True,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  sublinear_tf=False,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, use_idf=True,\n",
       "                                  vocabulary=None))],\n",
       "          verbose=False),\n",
       " 'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'process__memory': None,\n",
       " 'process__steps': [('selector', TextSelector(text='text')),\n",
       "  ('td_idf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None))],\n",
       " 'process__verbose': False,\n",
       " 'process__selector': TextSelector(text='text'),\n",
       " 'process__td_idf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'process__selector__text': 'text',\n",
       " 'process__td_idf__analyzer': 'word',\n",
       " 'process__td_idf__binary': False,\n",
       " 'process__td_idf__decode_error': 'strict',\n",
       " 'process__td_idf__dtype': numpy.float64,\n",
       " 'process__td_idf__encoding': 'utf-8',\n",
       " 'process__td_idf__input': 'content',\n",
       " 'process__td_idf__lowercase': True,\n",
       " 'process__td_idf__max_df': 1.0,\n",
       " 'process__td_idf__max_features': None,\n",
       " 'process__td_idf__min_df': 1,\n",
       " 'process__td_idf__ngram_range': (1, 1),\n",
       " 'process__td_idf__norm': 'l2',\n",
       " 'process__td_idf__preprocessor': None,\n",
       " 'process__td_idf__smooth_idf': True,\n",
       " 'process__td_idf__stop_words': None,\n",
       " 'process__td_idf__strip_accents': None,\n",
       " 'process__td_idf__sublinear_tf': False,\n",
       " 'process__td_idf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'process__td_idf__tokenizer': None,\n",
       " 'process__td_idf__use_idf': True,\n",
       " 'process__td_idf__vocabulary': None,\n",
       " 'clf__alpha': 1.0,\n",
       " 'clf__class_prior': None,\n",
       " 'clf__fit_prior': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'process__vectorizer__vectorizer': [TfidfVectorizer(), CountVectorizer()]\n",
    "    }\n",
    "]\n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, cv=cv_sets, scoring=scoring, refit='my_precision', n_jobs=4, return_train_score=False, verbose=3)\n",
    "gscv.fit(samsung_x.reset_index(drop=True), y_process.transform(samsung_y).reset_index(drop=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
